{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepART Drafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (mnist_train, mnist_test), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "469/469 [==============================] - 3s 3ms/step - loss: 0.3598 - sparse_categorical_accuracy: 0.9006 - val_loss: 0.2013 - val_sparse_categorical_accuracy: 0.9428\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1701 - sparse_categorical_accuracy: 0.9515 - val_loss: 0.1441 - val_sparse_categorical_accuracy: 0.9577\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1216 - sparse_categorical_accuracy: 0.9654 - val_loss: 0.1095 - val_sparse_categorical_accuracy: 0.9665\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0932 - sparse_categorical_accuracy: 0.9730 - val_loss: 0.0961 - val_sparse_categorical_accuracy: 0.9707\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0744 - sparse_categorical_accuracy: 0.9792 - val_loss: 0.0901 - val_sparse_categorical_accuracy: 0.9723\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0609 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0845 - val_sparse_categorical_accuracy: 0.9751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27ba8357670>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EWC + ART training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 28\n",
    "model_bu = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(dim, dim)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model_td = tf.keras.models.Sequential([\n",
    "  # tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Flatten(input_shape=(10,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(dim**2)\n",
    "])\n",
    "\n",
    "model_bu.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model_td.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_precision_matrices(model, task_set, num_batches=1, batch_size=32):\n",
    "#   task_set = task_set.repeat()\n",
    "#   precision_matrices = {n: tf.zeros_like(p.value()) for n, p in enumerate(model.trainable_variables)}\n",
    "\n",
    "#   for i, (imgs, labels) in enumerate(task_set.take(num_batches)):\n",
    "#     # We need gradients of model params\n",
    "#     with tf.GradientTape() as tape:\n",
    "#       # Get model predictions for each image\n",
    "#       preds = model(imgs)\n",
    "#       # Get the log likelihoods of the predictions\n",
    "#       ll = tf.nn.log_softmax(preds)\n",
    "#     # Attach gradients of ll to ll_grads\n",
    "#     ll_grads = tape.gradient(ll, model.trainable_variables)\n",
    "#     # Compute F_i as mean of gradients squared\n",
    "#     for i, g in enumerate(ll_grads):\n",
    "#       precision_matrices[i] += tf.math.reduce_mean(g ** 2, axis=0) / num_batches\n",
    "\n",
    "#   return precision_matrices\n",
    "\n",
    "# def compute_elastic_penalty(F, theta, theta_A, alpha=25):\n",
    "#   penalty = 0\n",
    "#   for i, theta_i in enumerate(theta):\n",
    "#     _penalty = tf.math.reduce_sum(F[i] * (theta_i - theta_A[i]) ** 2)\n",
    "#     penalty += _penalty\n",
    "#   return 0.5*alpha*penalty\n",
    "\n",
    "# def ewc_loss(labels, preds, model, F, theta_A):\n",
    "#   loss_b = model.loss(labels, preds)\n",
    "#   penalty = compute_elastic_penalty(F, model.trainable_variables, theta_A)\n",
    "#   return loss_b + penalty\n",
    "\n",
    "def l2_penalty(theta, theta_A):\n",
    "    penalty = 0\n",
    "    for i, theta_i in enumerate(theta):\n",
    "        penalty += tf.math.reduce_sum(tf.math.square(theta_i - theta_A[i]))\n",
    "    return 0.5*penalty\n",
    "\n",
    "def train_art(model_bu, model_td, train, test, epochs=6):\n",
    "    # We'll only compute Fisher once, you can do it whenever\n",
    "    # F = compute_precision_matrices(model, task_A_set, num_batches=1000)\n",
    "\n",
    "    theta_bu = {n: p.value() for n, p in enumerate(model_bu.trainable_variables.copy())}\n",
    "    theta_td = {n: p.value() for n, p in enumerate(model_td.trainable_variables.copy())}\n",
    "    \n",
    "    accuracy_bu = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    loss_bu = tf.keras.metrics.SparseCategoricalCrossentropy('loss')\n",
    "\n",
    "    accuracy_td = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    loss_td = tf.keras.metrics.SparseCategoricalCrossentropy('loss')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        accuracy_bu.reset_states()\n",
    "        loss_bu.reset_states()\n",
    "        accuracy_td.reset_states()\n",
    "        loss_td.reset_states()\n",
    "\n",
    "        # for batch, (imgs, labels) in enumerate(train):\n",
    "        #     for img in imgs:\n",
    "        #         with tf.GradientTape() as tape:\n",
    "        #             pred = model_bu(img)\n",
    "        #             total_loss = model.loss(labels, preds) + l2_penalty(model.trainable_variables, theta_A)\n",
    "                # preds = model(imgs)\n",
    "    #         total_loss = model.loss(labels, preds) + l2_penalty(model.trainable_variables, theta_A)\n",
    "    #     grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    #     model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "    #     accuracy.update_state(labels, preds)\n",
    "    #     loss.update_state(labels, preds)\n",
    "    #     print(\"\\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f}\".format(\n",
    "    #         epoch+1, batch+1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end=''\n",
    "    #         )\n",
    "    #     print(\"\")\n",
    "\n",
    "train_art(model_bu, model_td, ds_train, ds_test, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def l2_penalty(theta, theta_A):\n",
    "#     penalty = 0\n",
    "#     for i, theta_i in enumerate(theta):\n",
    "#         _penalty = tf.math.reduce_sum((theta_i - theta_A[i]) ** 2)\n",
    "#         penalty += _penalty\n",
    "#     return 0.5*penalty\n",
    "\n",
    "# def train_art(model, train, test, epochs=6):\n",
    "#     #     theta_A = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}\n",
    "#     accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "#     loss = tf.keras.metrics.SparseCategoricalCrossentropy('loss')\n",
    "#     for epoch in range(epochs):\n",
    "#         accuracy.reset_states()\n",
    "#         loss.reset_states()\n",
    "#         for batch, (imgs, labels) in enumerate(task_B_train):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             preds = model(imgs)\n",
    "#             total_loss = model.loss(labels, preds) + l2_penalty(model.trainable_variables, theta_A)\n",
    "#         grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "#         model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "#         accuracy.update_state(labels, preds)\n",
    "#         loss.update_state(labels, preds)\n",
    "#         print(\"\\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f}\".format(\n",
    "#             epoch+1, batch+1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end=''\n",
    "#             )\n",
    "#         print(\"\")\n",
    "  \n",
    "\n",
    "# def train_with_l2(model, task_A_train, task_B_train, task_A_test, task_B_test, epochs=6):\n",
    "#     # First we're going to fit to task A and retain a copy of parameters trained on Task A\n",
    "#     model.fit(task_A_train, epochs=epochs)\n",
    "#     theta_A = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}\n",
    "\n",
    "#     print(\"Task A accuracy after training on Task A: {}\".format(evaluate(model, task_A_test)))\n",
    "\n",
    "#     accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "#     loss = tf.keras.metrics.SparseCategoricalCrossentropy('loss')\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         accuracy.reset_states()\n",
    "#         loss.reset_states()\n",
    "#         for batch, (imgs, labels) in enumerate(task_B_train):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             preds = model(imgs)\n",
    "#             total_loss = model.loss(labels, preds) + l2_penalty(model.trainable_variables, theta_A)\n",
    "#         grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "#         model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "#         accuracy.update_state(labels, preds)\n",
    "#         loss.update_state(labels, preds)\n",
    "#         print(\"\\rEpoch: {}, Batch: {}, Loss: {:.3f}, Accuracy: {:.3f}\".format(\n",
    "#             epoch+1, batch+1, loss.result().numpy(), accuracy.result().numpy()), flush=True, end=''\n",
    "#             )\n",
    "#         print(\"\")\n",
    "  \n",
    "#     print(\"Task B accuracy after training trained model on Task B: {}\".format(evaluate(model, task_B_test)))\n",
    "#     print(\"Task A accuracy after training trained model on Task B: {}\".format(evaluate(model, task_A_test)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40ea4657e151deccd26e710afdc23502568d00c9dc74ee869c6dd1f0e477bcf5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('deepart2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
